{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOcE1Bo8eqB2yDy6ry1WabZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Import libraries"],"metadata":{"id":"Lx8qDM7u1o0j"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aWadbYdl19sJ","executionInfo":{"status":"ok","timestamp":1701425195666,"user_tz":-60,"elapsed":16574,"user":{"displayName":"Rob van der Wielen","userId":"13901213600166728231"}},"outputId":"fa88afba-811b-4821-d099-c7fdbaff0386"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import requests\n","import zipfile\n","import io\n","import pandas as pd\n","import glob\n","from tqdm import tqdm\n","import os\n","\n"],"metadata":{"id":"Sh7VaPBH0e-p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Loading in data"],"metadata":{"id":"egt2I_Ew0Q5q"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"e6c86b97"},"outputs":[],"source":["urls = [\n","    \"https://s3.amazonaws.com/tripdata/201901-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/201902-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/201903-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/201904-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/201905-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/201906-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/201907-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/201908-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/201909-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/201910-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/201911-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/201912-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202001-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202002-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202003-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202004-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202005-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202006-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202007-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202008-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202009-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202010-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202011-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202012-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202101-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202102-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202103-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202104-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202105-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202106-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202107-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202108-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202109-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202110-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202111-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202112-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202201-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202202-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202203-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202204-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202205-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202206-citbike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202207-citbike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202208-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202209-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202210-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202211-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202212-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202301-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202302-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202303-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202304-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202305-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202306-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202307-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202308-citibike-tripdata.csv.zip\",\n","    \"https://s3.amazonaws.com/tripdata/202309-citibike-tripdata.csv.zip\",\n","]\n","\n","# Split the URLs into batches of 5\n","batch_size = 12\n","url_batches = [urls[i:i + batch_size] for i in range(0, len(urls), batch_size)]\n","\n","for batch_num, batch in enumerate(tqdm(url_batches, desc=\"Processing batches\", unit=\"batch\"), start=start_batch):\n","    combined_data = []\n","\n","    for url in tqdm(batch, desc=f\"Processing files in batch {batch_num+1}\", unit=\"file\"):\n","        response = requests.get(url)\n","        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n","            with z.open(z.namelist()[0]) as f:\n","                print(url)\n","                df = pd.read_csv(f)\n","                combined_data.append(df)\n","\n","    # Combine dataframes of the current batch\n","    batch_df = pd.concat(combined_data, ignore_index=True)\n","\n","\n","    # Save to an intermediate CSV file\n","    batch_df.to_csv(f\"batch_{batch_num+1}_citibike_data.csv\", index=False)"]},{"cell_type":"markdown","source":["# Generating batch of data and aggregating directly to hourly level"],"metadata":{"id":"DX1S9iz_0U6w"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7a146939"},"outputs":[],"source":["csv_files = [\n","    \"batch_5_citibike_data.csv\",\n","    \"batch_6_citibike_data.csv\",\n","    \"batch_7_citibike_data.csv\",\n","    \"batch_8_citibike_data.csv\",\n","    \"batch_9_citibike_data.csv\",\n","    \"batch_10_citibike_data.csv\",\n","    \"batch_11_citibike_data.csv\",\n","    \"batch_12_citibike_data.csv\"\n","]\n","combined_data = []\n","\n","# Load each CSV file and append to the combined_data list\n","for file in csv_files:\n","    df = pd.read_csv(file)\n","    combined_data.append(df)\n","    print(\"Done\")\n","\n","    # Standardize column names\n","    df.columns = df.columns.str.replace(' ', '_').str.upper()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1f4bd2b2"},"outputs":[],"source":["\n","# Function to standardize column names\n","def standardize_columns(df):\n","    column_mapping = {\n","        'STARTTIME': 'STARTED_AT',\n","        'STOPTIME': 'ENDED_AT',\n","        'START_STATION_ID': 'START_STATION_ID',\n","        'START_STATION_NAME': 'START_STATION_NAME',\n","        'START_STATION_LATITUDE': 'START_LAT',\n","        'START_STATION_LONGITUDE': 'START_LNG',\n","        'END_STATION_ID': 'END_STATION_ID',\n","        'END_STATION_NAME': 'END_STATION_NAME',\n","        'END_STATION_LATITUDE': 'END_LAT',\n","        'END_STATION_LONGITUDE': 'END_LNG'\n","    }\n","    df.rename(columns=column_mapping, inplace=True)\n","    return df\n","\n","# Function to process and transform each dataframe\n","def process_dataframe(df):\n","    df = standardize_columns(df)\n","\n","    # Convert the 'STARTED_AT' and 'ENDED_AT' columns to datetime format\n","    df['STARTED_AT'] = pd.to_datetime(df['STARTED_AT'])\n","    df['ENDED_AT'] = pd.to_datetime(df['ENDED_AT'])\n","\n","    # Extract the hour and day from the 'STARTED_AT' and 'ENDED_AT' columns\n","    df['start_hour'] = df['STARTED_AT'].dt.hour\n","    df['start_day'] = df['STARTED_AT'].dt.date\n","    df['end_hour'] = df['ENDED_AT'].dt.hour\n","    df['end_day'] = df['ENDED_AT'].dt.date\n","\n","    # Aggregate data for pickups (start) based on start_day, start_hour, and start_station_id\n","    start_agg = df.groupby(['START_STATION_ID', 'start_day', 'start_hour']).size().reset_index(name='start_count')\n","\n","    # Aggregate data for drop-offs (end) based on end_day, end_hour, and end_station_id\n","    end_agg = df.groupby(['END_STATION_ID', 'end_day', 'end_hour']).size().reset_index(name='end_count')\n","\n","    # Merge the aggregated data on station ID, day, and hour\n","    merged_agg = pd.merge(start_agg, end_agg,\n","                          left_on=['START_STATION_ID', 'start_day', 'start_hour'],\n","                          right_on=['END_STATION_ID', 'end_day', 'end_hour'],\n","                          how='outer').fillna(0)\n","\n","    # Rename columns for clarity and drop unnecessary columns\n","    merged_agg.rename(columns={\n","        'START_STATION_ID': 'station_id',\n","        'start_day': 'day',\n","        'start_hour': 'hour'\n","    }, inplace=True)\n","    merged_agg.drop(columns=['END_STATION_ID', 'end_day', 'end_hour'], inplace=True)\n","\n","    # Create a dataframe with unique station IDs and their corresponding longitude, latitude, and station name\n","    station_data = df.groupby('START_STATION_ID').agg({\n","        'START_LAT': 'first',\n","        'START_LNG': 'first',\n","        'START_STATION_NAME': 'first'\n","    }).reset_index()\n","\n","    # Rename columns for clarity\n","    station_data.rename(columns={\n","        'START_STATION_ID': 'station_id',\n","        'START_LAT': 'latitude',\n","        'START_LNG': 'longitude',\n","        'START_STATION_NAME': 'station_name'\n","    }, inplace=True)\n","\n","    # Merge the aggregated data with station data\n","    final_data = pd.merge(merged_agg, station_data, on='station_id', how='left')\n","\n","    # Remove rows with 0 values in 'station_id' or 'hour' columns\n","    final_data_cleaned = final_data[~((final_data['station_id'] == 0))]\n","\n","    return final_data_cleaned\n","\n","# Process each dataframe in combined_data and save to temporary CSV\n","temp_files = []\n","for idx, df in enumerate(combined_data):\n","    processed_df = process_dataframe(df)\n","    temp_filename = f\"temp_file_{idx}.csv\"\n","    processed_df.to_csv(temp_filename, index=False)\n","    temp_files.append(temp_filename)\n","    print(\"Done\")\n","\n","# Append each temporary CSV to the final CSV\n","final_filename = \"final_selected_combined_citibike_data.csv\"\n","with open(final_filename, 'w') as final_file:\n","    for idx, temp_file in enumerate(temp_files):\n","        with open(temp_file, 'r') as f:\n","            if idx == 0:  # Write header only for the first file\n","                final_file.write(f.readline())\n","            else:\n","                f.readline()  # Skip header for subsequent files\n","            final_file.writelines(f.readlines())\n","        os.remove(temp_file)  # Delete the temporary file\n","    print(\"Done\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bH_R3bUWPWU8"},"outputs":[],"source":["new_york_data = pd.read_csv(\"/content/final_selected_combined_citibike_data.csv\")"]}]}
