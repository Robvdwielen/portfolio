{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1WSwYiqs3TR8ywOob3SHQmkpUXAbjSfiG","authorship_tag":"ABX9TyPsYxFl7CnM5YW8V6Td0lTZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"GHS4k4odkdKs"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, concatenate, Flatten, Embedding, TimeDistributed, RepeatVector\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras import backend as K\n","import os\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from math import sqrt\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9LSS1jcJLkAh"},"outputs":[],"source":["import pandas as pd\n","merged_data_short = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/merged_data_short.csv\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-UnLy5IyLl4j"},"source":["# Generating and splitting data for models"]},{"cell_type":"markdown","source":["## Removing 50 sampled satation"],"metadata":{"id":"N2plsxf5k3EA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WN8KCFzpmoxO"},"outputs":[],"source":["# Define aggregation dictionary for variables other than 'start_count'\n","agg_funcs = {col: 'first' for col in merged_data_short.columns if col not in ['station_id_encoded', 'hour', 'date', 'start_count']}\n","agg_funcs['start_count'] = 'sum'\n","\n","# Grouping the data\n","merged_data_short = merged_data_short.groupby(['station_id_encoded', 'hour', 'date'], as_index=False).agg(agg_funcs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CAWihvTKcuTu"},"outputs":[],"source":["random_stations = merged_data_short['station_id_encoded'].drop_duplicates().sample(n=50, random_state=42)\n","\n","# Exclude these stations from the dataset\n","merged_data_short_sample = merged_data_short[~merged_data_short['station_id_encoded'].isin(random_stations)]\n"]},{"cell_type":"markdown","source":["## Adding dummy time slots and weekend variables"],"metadata":{"id":"ecoKFk-rmxPd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MsLoz7_bU1M6"},"outputs":[],"source":["merged_data_short_sample['date'] = pd.to_datetime(merged_data_short_sample['date'])\n","\n","# Define the function to create dummy variables for time of day\n","def time_of_day_dummy(hour):\n","    if 0 <= hour < 7:\n","        return 'early_morning'\n","    elif 7 <= hour < 9:\n","        return 'morning_rush'\n","    elif 9 <= hour < 12:\n","        return 'late_morning'\n","    elif 12 <= hour < 16:\n","        return 'lunch_time'\n","    elif 16 <= hour < 19:\n","        return 'afternoon_rush'\n","    elif 19 <= hour <= 23:\n","        return 'evening'\n","    else:\n","        return 'invalid_hour'\n","\n","# Apply the function to create a new column with time of day categories\n","merged_data_short_sample['time_of_day'] = merged_data_short_sample['hour'].apply(time_of_day_dummy)\n","\n","# Now we create dummy variables from the 'time_of_day' column\n","time_of_day_dummies = pd.get_dummies(merged_data_short_sample['time_of_day'], prefix='dummy')\n","\n","# Join the dummy variables with the main dataframe\n","merged_data_short_sample = merged_data_short_sample.join(time_of_day_dummies)\n","\n","# Drop the 'time_of_day' column as it's no longer needed\n","merged_data_short_sample.drop('time_of_day', axis=1, inplace=True)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1O3mBGXNWmV9"},"outputs":[],"source":["# Define a function to identify weekends\n","def is_weekend(date):\n","    if date.weekday() >= 5:\n","        return 1\n","    else:\n","        return 0\n","\n","# Apply the function to create the weekend dummy variable\n","merged_data_short_sample['weekend_dummy'] = merged_data_short_sample['date'].apply(is_weekend)"]},{"cell_type":"markdown","source":["## Splitting and standardizing"],"metadata":{"id":"GOCrLKVyk6-B"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"p9gvaZ2bLkAp"},"outputs":[],"source":["# Sort by 'date'\n","merged_data_short_sorted = merged_data_short_sample.sort_values(by=['date', 'station_id_encoded', 'hour'])\n","\n","# Drop columns not used for features or target\n","merged_data_short_sorted.drop(columns=[\"station_name\", \"short_name\"], inplace=True)\n","\n","# Reset index after sorting\n","merged_data_short_sorted.reset_index(drop=True, inplace=True)\n","\n","# Split sorted data into features (X) and target (y)\n","X_sorted = merged_data_short_sorted.drop('start_count', axis=1)\n","y_sorted = merged_data_short_sorted['start_count']\n","\n","# Split data on dates\n","train_end_index = X_sorted[X_sorted['date'] == '2022-11-30'].index[-1]\n","val_end_index = X_sorted[X_sorted['date'] == '2023-03-10'].index[-1]\n","\n","# Split the data manually\n","X_train = X_sorted.iloc[:train_end_index]\n","y_train = y_sorted.iloc[:train_end_index]\n","X_val = X_sorted.iloc[train_end_index:val_end_index]\n","y_val = y_sorted.iloc[train_end_index:val_end_index]\n","X_test = X_sorted.iloc[val_end_index:]\n","y_test = y_sorted.iloc[val_end_index:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tDEAhZSkLkAq"},"outputs":[],"source":["X_train.drop(columns=[\"date\", \"capacity\"], inplace=True)\n","X_val.drop(columns=[\"date\", \"capacity\"], inplace=True)\n","X_test.drop(columns=[\"date\", \"capacity\"], inplace=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PptKBzIRLkAq"},"outputs":[],"source":["# Feature columns to be standardized\n","feature_columns = [\"hour\", \"longitude\", \"latitude\", \"temperature_2m (°C)\",\n","                   \"relativehumidity_2m (%)\", \"precipitation (mm)\", \"snowfall (cm)\",\n","                   \"cloudcover (%)\", \"direct_radiation (W/m²)\", \"windspeed_10m (km/h)\",\n","                   \"bike_lane_length_km\", \"restaurants_count\", \"rail_stations_count\",\n","                   \"universities_count\", \"bus_stations_count\", \"businesses_count\", \"parks_count\"]\n","\n","# Create the Normalization layer and adapt it to the training data\n","standardizer = tf.keras.layers.Normalization(axis=-1)\n","standardizer.adapt(X_train[feature_columns])\n","\n","# Apply the standardizer to the training data\n","X_train[feature_columns] = standardizer(X_train[feature_columns].values)\n","\n","# Apply the standardizer to the validation data\n","X_val[feature_columns] = standardizer(X_val[feature_columns].values)\n","\n","# Apply the standardizer to the test data\n","X_test[feature_columns] = standardizer(X_test[feature_columns].values)\n","\n","\n","# Adapt the standardizer to the training data target variable\n","target_standardizer = tf.keras.layers.Normalization(axis=-1)\n","\n","target_standardizer.adapt(y_train.to_frame())\n","\n","# Apply the standardizer to the training data target variable\n","y_train = target_standardizer(y_train.to_frame()).numpy().flatten()\n","\n","# Apply the standardizer to the validation data target variable\n","y_val = target_standardizer(y_val.to_frame()).numpy().flatten()\n","\n","# Apply the standardizer to the test data target variable\n","y_test = target_standardizer(y_test.to_frame()).numpy().flatten()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sC_pOjck33zT"},"outputs":[],"source":["y_train = pd.DataFrame(y_train, columns=['start_count'])\n","y_test = pd.DataFrame(y_test, columns=['start_count'])\n","y_val = pd.DataFrame(y_val, columns=['start_count'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1701424200805,"user":{"displayName":"Rob van der Wielen","userId":"16334711937746181770"},"user_tz":-60},"id":"f0jQuNtN0h0e","outputId":"d108a7b2-a2f1-490c-c531-16b666c56580"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["214"]},"metadata":{},"execution_count":116}],"source":["# Number of unique stations in the dataset\n","num_stations = X_val['station_id_encoded'].nunique()\n","\n","# Maximum value of station ID (needed to define the input dimension of the embedding layer)\n","max_station_id = int(X_val['station_id_encoded'].max()) + 1\n","\n","# Define the embedding size\n","# Rule of thumb: min(50, number of categories/2)\n","embedding_size = min(50, num_stations // 2)\n","num_stations"]},{"cell_type":"markdown","metadata":{"id":"oZuv2SHc7vp-"},"source":["# Batch generator with ID's"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pFryPQ8Tvh8P"},"outputs":[],"source":["station_ids_train = X_train['station_id_encoded']\n","station_ids_val = X_val['station_id_encoded']\n","station_ids_test = X_test['station_id_encoded']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LD06c7levsjE"},"outputs":[],"source":["def modified_batch_generator(X, y, station_ids, time_steps=24, batch_size=256, infinite_loop=True):\n","    total_size = len(X)\n","    start_idx = 0\n","\n","    while True:\n","        X_batch = np.zeros((batch_size, time_steps, X.shape[1]))\n","        station_id_batch = np.zeros((batch_size, 1))\n","        y_batch = np.zeros((batch_size, time_steps, 1))\n","\n","        for i in range(batch_size):\n","            if start_idx + time_steps <= total_size:\n","                X_seq = X.iloc[start_idx:start_idx + time_steps]\n","                y_seq = y.iloc[start_idx:start_idx + time_steps]\n","                station_id = station_ids.iloc[start_idx + time_steps - 1]\n","\n","                # Padding in case of insufficient data\n","                X_pad = np.zeros((time_steps, X.shape[1]))\n","                y_pad = np.zeros((time_steps, 1))\n","                X_pad[:len(X_seq)] = X_seq.values\n","                y_pad[:len(y_seq)] = y_seq.values.reshape(-1, 1)\n","\n","                X_batch[i] = X_pad\n","                station_id_batch[i] = station_id\n","                y_batch[i] = y_pad\n","                start_idx += 1\n","            else:\n","                if infinite_loop:\n","                    start_idx = 0\n","                else:\n","                    break\n","\n","        yield ([X_batch, station_id_batch], y_batch)\n","\n","        if not infinite_loop and start_idx + time_steps > total_size:\n","            break\n","\n","# Recreate generators with the modified batch generator\n","train_gen = modified_batch_generator(X_train, y_train, station_ids_train, time_steps=24, batch_size=256, infinite_loop=True)\n","val_gen = modified_batch_generator(X_val, y_val, station_ids_val, time_steps=24, batch_size=256, infinite_loop=True)\n","test_gen = modified_batch_generator(X_test, y_test, station_ids_test, time_steps=24, batch_size=256, infinite_loop=False)\n","\n","batch_size = 256\n","# Get the next batch from the generator\n","batch_output = next(train_gen)\n","\n","# Extract station_id_batch, X_batch, and y_batch\n","X_batch, station_id_batch = batch_output[0]\n","y_batch = batch_output[1]\n","\n","# Print shapes\n","print(\"Station ID Batch Shape:\", station_id_batch.shape)\n","print(\"X Batch Shape:\", X_batch.shape)\n","print(\"y Batch Shape:\", y_batch.shape)"]},{"cell_type":"markdown","metadata":{"id":"n0IDjXV_G4z_"},"source":["# CNN-LSTM Model with capacity and no split\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":458,"status":"ok","timestamp":1701335508205,"user":{"displayName":"Rob van der Wielen","userId":"16334711937746181770"},"user_tz":-60},"id":"yBUYaOeVHCdL","outputId":"3a731fb2-e6b8-437c-be65-53cdfc7c82ae"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_1 (InputLayer)        [(None, 24, 48)]             0         []                            \n","                                                                                                  \n"," conv1d (Conv1D)             (None, 24, 64)               15424     ['input_1[0][0]']             \n","                                                                                                  \n"," dropout (Dropout)           (None, 24, 64)               0         ['conv1d[0][0]']              \n","                                                                                                  \n"," input_2 (InputLayer)        [(None, 1)]                  0         []                            \n","                                                                                                  \n"," lstm (LSTM)                 (None, 24, 70)               37800     ['dropout[0][0]']             \n","                                                                                                  \n"," embedding (Embedding)       (None, 1, 50)                13250     ['input_2[0][0]']             \n","                                                                                                  \n"," dropout_1 (Dropout)         (None, 24, 70)               0         ['lstm[0][0]']                \n","                                                                                                  \n"," flatten (Flatten)           (None, 50)                   0         ['embedding[0][0]']           \n","                                                                                                  \n"," time_distributed (TimeDist  (None, 24, 64)               4544      ['dropout_1[0][0]']           \n"," ributed)                                                                                         \n","                                                                                                  \n"," repeat_vector (RepeatVecto  (None, 24, 50)               0         ['flatten[0][0]']             \n"," r)                                                                                               \n","                                                                                                  \n"," concatenate (Concatenate)   (None, 24, 114)              0         ['time_distributed[0][0]',    \n","                                                                     'repeat_vector[0][0]']       \n","                                                                                                  \n"," time_distributed_1 (TimeDi  (None, 24, 64)               7360      ['concatenate[0][0]']         \n"," stributed)                                                                                       \n","                                                                                                  \n"," time_distributed_2 (TimeDi  (None, 24, 1)                65        ['time_distributed_1[0][0]']  \n"," stributed)                                                                                       \n","                                                                                                  \n","==================================================================================================\n","Total params: 78443 (306.42 KB)\n","Trainable params: 78443 (306.42 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}],"source":["from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, concatenate, Conv1D, Flatten, Embedding, TimeDistributed, RepeatVector\n","from tensorflow.keras import regularizers\n","\n","def rmse(y_true, y_pred):\n","    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n","\n","\n","cnn_lstm_input_shape = (24, X_batch.shape[2])\n","station_id_input_shape = (1,)\n","\n","def build_cnn_lstm_model(cnn_lstm_input_shape, station_id_input_shape):\n","    cnn_lstm_input = Input(shape=cnn_lstm_input_shape)\n","    conv1 = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(cnn_lstm_input)\n","    lstm = Dropout(rate=0.4)(conv1)\n","    lstm1 = LSTM(units=70, activation='relu', return_sequences=True)(lstm)\n","    lstm2 = Dropout(rate=0.4)(lstm1)\n","\n","    station_id_input = Input(shape=station_id_input_shape)\n","    station_id_embedding = Embedding(input_dim=max_station_id + 1, output_dim=embedding_size)(station_id_input)\n","    station_id_embedding = Flatten()(station_id_embedding)\n","    station_id_embedding = RepeatVector(24)(station_id_embedding)  # Repeat to match time steps\n","\n","    combined = concatenate([lstm2, station_id_embedding], axis=-1)\n","    time_distributed_output = TimeDistributed(Dense(64))(combined)\n","    final_output = TimeDistributed(Dense(1))(time_distributed_output)  # Predicting for each time step\n","\n","    optimizer = RMSprop(learning_rate=0.0001)\n","\n","    model = Model(inputs=[cnn_lstm_input, station_id_input], outputs=final_output)\n","    model.compile(optimizer=optimizer, loss='mse', metrics=['mae', rmse])\n","\n","    return model\n","\n","cnn_lstm_model_no_cap = build_cnn_lstm_model(cnn_lstm_input_shape, station_id_input_shape)\n","cnn_lstm_model_no_cap.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tXM0TN7pHEPt"},"outputs":[],"source":["early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)"]},{"cell_type":"markdown","source":["# Model with checkpoints to save during running"],"metadata":{"id":"Ej5hWVLilDwv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ePe-GfNzz9ew"},"outputs":[],"source":["checkpoint_dir = 'checkpoint_directory'\n","list_of_files = os.listdir(checkpoint_dir)\n","# Filter out files that match your checkpoint pattern and sort them\n","checkpoint_files = sorted([f for f in list_of_files if 'model_cnn-lstm_no_cap' in f])\n","\n","if checkpoint_files:\n","    last_checkpoint = checkpoint_files[-1]\n","    last_model_path = os.path.join(checkpoint_dir, last_checkpoint)\n","    # Load the last model with the custom rmse function\n","    cnn_lstm_model_no_cap = load_model(last_model_path, custom_objects={'rmse': rmse})\n","else:\n","    print(\"No checkpoint found. Please train the model from scratch.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wnj7dyGNHHcx"},"outputs":[],"source":["# Configure ModelCheckpoint\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","checkpoint = ModelCheckpoint(\n","    'checkpoint_directory/model_cnn-lstm_no_cap{epoch:02d}-{val_loss:.2f}.h5',\n","    monitor='val_loss',\n","    verbose=1,\n","    save_best_only=True,\n","    mode='min'\n",")\n","\n","steps_per_epoch = len(X_train) // batch_size\n","# Calculate the number of steps in the validation generator\n","validation_steps = len(X_val) // batch_size\n","\n","history = cnn_lstm_model_no_cap.fit(\n","    x=train_gen,\n","    steps_per_epoch=steps_per_epoch,\n","    epochs=50,\n","    validation_data=val_gen,\n","    validation_steps=validation_steps,\n","    callbacks=[early_stopping, checkpoint]\n",")"]},{"cell_type":"markdown","source":["# Generating predictions"],"metadata":{"id":"cGV6Za6slMQV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"iWtmcaG3mjhH"},"outputs":[],"source":["from tensorflow.keras.models import load_model\n","\n","# Replace this path with the path to your model file\n","model_path = \"/content/drive/MyDrive/Colab Notebooks/checkpoint_directory/model_cnn-lstm_no_cap28-0.24.h5\"\n","\n","# Load the model\n","cnn_lstm_model_no_cap = load_model(model_path, custom_objects={'rmse': rmse})\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VOy7zrzgzIfE"},"outputs":[],"source":["def reshape_and_pad_data(X, y, time_steps=24):\n","    # Calculate the padding size\n","    pad_size = time_steps - (X.shape[0] % time_steps)\n","    pad_size = pad_size if pad_size != time_steps else 0\n","\n","    # Pad the data\n","    X_padded = np.vstack([X, X.iloc[:pad_size]])\n","    y_padded = np.vstack([y, y.iloc[:pad_size]])\n","\n","    # Reshape the data\n","    X_reshaped = X_padded.reshape((-1, time_steps, X.shape[1]))\n","    y_reshaped = y_padded.reshape((-1, time_steps, 1))  # Add an extra dimension for y\n","\n","    return X_reshaped, y_reshaped\n","\n","# Reshape and pad test data\n","X_test_reshaped, y_test_reshaped = reshape_and_pad_data(X_test, y_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BTBi3zF8z-3H"},"outputs":[],"source":["def replicate_and_reshape_station_ids(station_ids, num_sequences):\n","    # Calculate the exact number of replications needed\n","    num_replications = int(np.ceil(num_sequences / len(station_ids)))\n","\n","    # Replicate the station IDs\n","    station_ids_replicated = np.repeat(station_ids, num_replications)\n","\n","    # Trim the replicated array to match the exact number of sequences\n","    station_ids_trimmed = station_ids_replicated[:num_sequences]\n","\n","    # Reshape the station IDs to match the required format\n","    station_ids_reshaped = station_ids_trimmed.reshape((num_sequences, 1))\n","\n","    return station_ids_reshaped\n","\n","# Use the function to reshape station_id_batch\n","num_sequences = X_test_reshaped.shape[0]\n","station_id_batch_reshaped = replicate_and_reshape_station_ids(station_id_batch, num_sequences)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mcdbrtmv1X3R"},"outputs":[],"source":["# Now station_id_batch_reshaped should be ready for use in prediction\n","test_predictions_batch = cnn_lstm_model_no_cap.predict([X_test_reshaped, station_id_batch_reshaped])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xrLcomBA3gba"},"outputs":[],"source":["# Flatten the last two dimensions for both predictions and actuals\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from math import sqrt\n","y_test_flat = y_test_reshaped.reshape(-1)  # Reshape to (51240 * 24,)\n","test_predictions_flat = test_predictions_batch.reshape(-1)  # Reshape to (51240 * 24,)\n","\n","# Calculate MAE and RMSE\n","test_mae = mean_absolute_error(y_test_flat, test_predictions_flat)\n","test_rmse = sqrt(mean_squared_error(y_test_flat, test_predictions_flat))\n","\n","print('Test MAE:', test_mae)\n","print('Test RMSE:', test_rmse)\n"]},{"cell_type":"markdown","source":["# Export results"],"metadata":{"id":"Q3lFb2f1lQK0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UDrIY7OKxyAh"},"outputs":[],"source":["def unpad_predictions(predictions, original_data, time_steps=24):\n","    # Calculate the pad size used during padding\n","    pad_size = time_steps - (original_data.shape[0] % time_steps)\n","    pad_size = pad_size if pad_size != time_steps else 0\n","\n","    # Remove the padding\n","    predictions_unpadded = predictions[:-pad_size] if pad_size != 0 else predictions\n","\n","    return predictions_unpadded\n","\n","# Unpad the predictions\n","unpadded_predictions = unpad_predictions(test_predictions_flat, y_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Is6pzBRLyhTC"},"outputs":[],"source":["# Convert the numpy array to a pandas DataFrame\n","predictions_standardized_cnn_lstm_no_cap= pd.DataFrame(unpadded_predictions, columns=['prediction'])\n","\n","# Export to a CSV file\n","predictions_standardized_cnn_lstm_no_cap.to_csv('/content/drive/MyDrive/Colab Notebooks/predictions_standardized_cnn_lstm_no_cap', index=False)\n"]},{"cell_type":"markdown","source":["# Export unstandardized results"],"metadata":{"id":"Igbt4ERtlTIh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YQUkQe0GxyAp"},"outputs":[],"source":["# Retrieve the mean and variance from the target_standardizer\n","mean, variance = target_standardizer.mean.numpy(), target_standardizer.variance.numpy()\n","\n","# Compute the standard deviation from the variance\n","std_dev = np.sqrt(variance)\n","\n","# Extract single value for mean and standard deviation\n","mean_value = mean.item()  # Assuming mean is a numpy array with a single element\n","std_dev_value = np.sqrt(variance).item()  # Assuming variance is a numpy array with a single element\n","\n","# Apply the inverse transformation formula to unstandardize adjusted_train_predictions_flat\n","predictions_unstandardized_cnn_lstm_no_cap = unpadded_predictions * std_dev_value + mean_value\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lPC_51qsxyAp"},"outputs":[],"source":["import pandas as pd\n","\n","# Convert the numpy array to a pandas DataFrame\n","predictions_unstandardized_cnn_lstm_no_cap = pd.DataFrame(predictions_unstandardized_cnn_lstm_no_cap, columns=['prediction'])\n","\n","# Export to a CSV file\n","predictions_unstandardized_cnn_lstm_no_cap.to_csv('/content/drive/MyDrive/Colab Notebooks/predictions_standardized_cnn_lstm_no_cap', index=False)\n"]}]}
