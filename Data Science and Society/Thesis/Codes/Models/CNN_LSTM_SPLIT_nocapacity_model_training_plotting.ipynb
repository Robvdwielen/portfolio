{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP6Pngt3iYGEfXlxCxvxQAo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Loading libraries"],"metadata":{"id":"L1LwD-QYmcbj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GHS4k4odkdKs"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, concatenate, Flatten, Embedding, TimeDistributed, RepeatVector\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras import backend as K\n","import os\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from math import sqrt\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"markdown","metadata":{"id":"-UnLy5IyLl4j"},"source":["# Generating and splitting data for models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9LSS1jcJLkAh"},"outputs":[],"source":["import pandas as pd\n","merged_data_short = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/merged_data_short.csv\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"C5uIeaaqwEw2"},"source":["#CNN-LSTM MODEL WITH SPLIT"]},{"cell_type":"markdown","source":["## Removing 50 sampled satation"],"metadata":{"id":"N2plsxf5k3EA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vLvspuFAJgEC"},"outputs":[],"source":["# Define aggregation dictionary for variables other than 'start_count'\n","agg_funcs = {col: 'first' for col in merged_data_short.columns if col not in ['station_id_encoded', 'hour', 'date', 'start_count']}\n","agg_funcs['start_count'] = 'sum'  # Summing up the 'start_count'\n","\n","# Grouping the data\n","merged_data_short = merged_data_short.groupby(['station_id_encoded', 'hour', 'date'], as_index=False).agg(agg_funcs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bSJEf6K5wSSY"},"outputs":[],"source":["# Sampling 50 unique station IDs at random\n","random_stations = aggregated_df['station_id_encoded'].drop_duplicates().sample(n=50, random_state=42)\n","\n","# Exclude these stations from the dataset\n","aggregated_df_sample = aggregated_df[~aggregated_df['station_id_encoded'].isin(random_stations)]\n"]},{"cell_type":"markdown","source":["# Generating generic model first"],"metadata":{"id":"Vn9GRyuuxxZg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y7r10eW2wSSY"},"outputs":[],"source":["aggregated_df = merged_data_short[[\"start_count\",\"date\", \"station_id_encoded\", \"hour\",\n","    \"temperature_2m (°C)\",\n","    \"relativehumidity_2m (%)\",\n","    \"precipitation (mm)\",\n","    \"snowfall (cm)\",\n","    \"cloudcover (%)\",\n","    \"IsHoliday\",\n","    \"direct_radiation (W/m²)\",\n","    \"windspeed_10m (km/h)\",\n","    \"year_2021\",\n","    \"year_2022\",\n","    \"year_2023\",\n","    \"month_name_April\",\n","    \"month_name_August\",\n","    \"month_name_December\",\n","    \"month_name_February\",\n","    \"month_name_January\",\n","    \"month_name_July\",\n","    \"month_name_June\",\n","    \"month_name_March\",\n","    \"month_name_May\",\n","    \"month_name_November\",\n","    \"month_name_October\",\n","    \"month_name_September\",\n","    \"day_of_week_Friday\",\n","    \"day_of_week_Monday\",\n","    \"day_of_week_Saturday\",\n","    \"day_of_week_Sunday\",\n","    \"day_of_week_Thursday\",\n","    \"day_of_week_Tuesday\",\n","    \"day_of_week_Wednesday\"]]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r5Ay_4JJFcPJ"},"outputs":[],"source":["aggregated_df_sample['date'] = pd.to_datetime(aggregated_df_sample['date'])\n","def time_of_day_dummy(hour):\n","    if 0 <= hour < 7:\n","        return 'early_morning'\n","    elif 7 <= hour < 9:\n","        return 'morning_rush'\n","    elif 9 <= hour < 12:\n","        return 'late_morning'\n","    elif 12 <= hour < 16:\n","        return 'lunch_time'\n","    elif 16 <= hour < 19:\n","        return 'afternoon_rush'\n","    elif 19 <= hour <= 23:\n","        return 'evening'\n","    else:\n","        return 'invalid_hour'  # Just in case there are any unexpected hour values\n","\n","# Apply the function to create a new column with time of day categories\n","aggregated_df_sample['time_of_day'] = aggregated_df_sample['hour'].apply(time_of_day_dummy)\n","\n","# Now we create dummy variables from the 'time_of_day' column\n","time_of_day_dummies = pd.get_dummies(aggregated_df_sample['time_of_day'], prefix='dummy')\n","\n","# Join the dummy variables with the main dataframe\n","aggregated_df_sample = aggregated_df_sample.join(time_of_day_dummies)\n","\n","# Drop the 'time_of_day' column as it's no longer needed\n","aggregated_df_sample.drop('time_of_day', axis=1, inplace=True)\n","\n","# Show the dataframe to confirm the changes\n","aggregated_df_sample.head()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tRYI1bagFew0"},"outputs":[],"source":["def is_weekend(date):\n","    if date.weekday() >= 5:\n","        return 1\n","    else:\n","        return 0\n","\n","# Apply the function to create the weekend dummy variable\n","aggregated_df_sample['weekend_dummy'] = aggregated_df_sample['date'].apply(is_weekend)\n"]},{"cell_type":"markdown","source":["## Splitting and standardizing data"],"metadata":{"id":"QO8nESfryBeq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KO1dOWs_wSSa"},"outputs":[],"source":["# Sort by 'date' (and other relevant columns if necessary)\n","merged_data_short_sorted = aggregated_df_sample.sort_values(by=['date', 'station_id_encoded'])\n","# Reset index after sorting\n","merged_data_short_sorted.reset_index(drop=True, inplace=True)\n","\n","# Split sorted data into features (X) and target (y)\n","X_sorted = merged_data_short_sorted.drop('start_count', axis=1)\n","y_sorted = merged_data_short_sorted['start_count']\n","\n","# Splitting data on date\n","train_end_index = X_sorted[X_sorted['date'] == '2022-11-30'].index[-1]\n","val_end_index = X_sorted[X_sorted['date'] == '2023-03-10'].index[-1]\n","\n","# Split the data manually\n","X_train = X_sorted.iloc[:train_end_index]\n","y_train = y_sorted.iloc[:train_end_index]\n","X_val = X_sorted.iloc[train_end_index:val_end_index]\n","y_val = y_sorted.iloc[train_end_index:val_end_index]\n","X_test = X_sorted.iloc[val_end_index:]\n","y_test = y_sorted.iloc[val_end_index:]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rg-WHjeKwSSa"},"outputs":[],"source":["X_train.drop(columns=[\"date\"], inplace=True)\n","X_val.drop(columns=[\"date\"], inplace=True)\n","X_test.drop(columns=[\"date\"], inplace=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TW9tP42iwSSb"},"outputs":[],"source":["# Feature columns to be standardized\n","feature_columns = [\"hour\", \"temperature_2m (°C)\",\n","                   \"relativehumidity_2m (%)\", \"precipitation (mm)\", \"snowfall (cm)\",\n","                   \"cloudcover (%)\", \"direct_radiation (W/m²)\", \"windspeed_10m (km/h)\"]\n","\n","# Create the Normalization layer and adapt it to the training data\n","standardizer = tf.keras.layers.Normalization(axis=-1)\n","standardizer.adapt(X_train[feature_columns])\n","\n","# Apply the standardizer to the training data\n","X_train[feature_columns] = standardizer(X_train[feature_columns].values)\n","\n","# Apply the standardizer to the validation data\n","X_val[feature_columns] = standardizer(X_val[feature_columns].values)\n","\n","# Apply the standardizer to the test data\n","X_test[feature_columns] = standardizer(X_test[feature_columns].values)\n","\n","\n","# Adapt the standardizer to the training data target variable\n","target_standardizer = tf.keras.layers.Normalization(axis=-1)\n","\n","target_standardizer.adapt(y_train.to_frame())\n","\n","# Apply the standardizer to the training data target variable\n","y_train = target_standardizer(y_train.to_frame()).numpy().flatten()\n","\n","# Apply the standardizer to the validation data target variable\n","y_val = target_standardizer(y_val.to_frame()).numpy().flatten()\n","\n","# Apply the standardizer to the test data target variable\n","y_test = target_standardizer(y_test.to_frame()).numpy().flatten()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J9vEqMtyL_WF"},"outputs":[],"source":["X_train.drop(columns=[\"station_id_encoded\"], inplace=True)\n","X_val.drop(columns=[\"station_id_encoded\"], inplace=True)\n","X_test.drop(columns=[\"station_id_encoded\"], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qPYvMbjtwSSb"},"outputs":[],"source":["y_train = pd.DataFrame(y_train, columns=['start_count'])\n","y_test = pd.DataFrame(y_test, columns=['start_count'])\n","y_val = pd.DataFrame(y_val, columns=['start_count'])"]},{"cell_type":"markdown","source":["## Modified batch generator\n"],"metadata":{"id":"4_Bn5RpfyNdH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"M_jpxntkSEJ9"},"outputs":[],"source":["def modified_batch_generator_without_station_ids(X, y, time_steps=24, batch_size=256, infinite_loop=True):\n","    total_size = len(X) - time_steps\n","    start_idx = 0\n","\n","    while True:\n","        X_batch = np.zeros((batch_size, time_steps, X.shape[1]))\n","        y_batch = np.zeros((batch_size, time_steps, 1))\n","\n","        for i in range(batch_size):\n","            if start_idx + time_steps <= total_size:\n","                X_batch[i] = X.iloc[start_idx:start_idx + time_steps].values\n","                y_batch[i] = y.iloc[start_idx:start_idx + time_steps].values.reshape(-1, 1)\n","                start_idx += 1\n","            else:\n","                if infinite_loop:\n","                    start_idx = 0\n","                else:\n","                    break\n","\n","        yield (X_batch, y_batch)\n","\n","        if not infinite_loop and start_idx + time_steps > total_size:\n","            break\n","\n","# Example usage\n","train_gen_without_station_ids = modified_batch_generator_without_station_ids(X_train, y_train, time_steps=24, batch_size=256, infinite_loop=True)\n","val_gen_without_station_ids = modified_batch_generator_without_station_ids(X_val, y_val, time_steps=24, batch_size=256, infinite_loop=True)\n","test_gen_without_station_ids = modified_batch_generator_without_station_ids(X_test, y_test, time_steps=24, batch_size=256, infinite_loop=False)\n","\n","# Get the next batch from the generator\n","batch_output = next(train_gen_without_station_ids)\n","\n","# Extract X_batch and y_batch\n","X_batch, y_batch = batch_output\n","\n","# Print shapes\n","print(\"X Batch Shape:\", X_batch.shape)\n","print(\"y Batch Shape:\", y_batch.shape)\n"]},{"cell_type":"markdown","source":["## Generic feature model"],"metadata":{"id":"gSxxo9asySID"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":726,"status":"ok","timestamp":1701356919499,"user":{"displayName":"Rob van der Wielen","userId":"16334711937746181770"},"user_tz":-60},"id":"L0BFqKB4PbNC","outputId":"447c1d27-445e-4055-ab46-770a3fd15d2f"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 24, 38)]          0         \n","                                                                 \n"," conv1d (Conv1D)             (None, 24, 64)            12224     \n","                                                                 \n"," lstm (LSTM)                 (None, 24, 70)            37800     \n","                                                                 \n"," dropout (Dropout)           (None, 24, 70)            0         \n","                                                                 \n"," time_distributed (TimeDist  (None, 24, 20)            1420      \n"," ributed)                                                        \n","                                                                 \n"," time_distributed_1 (TimeDi  (None, 24, 1)             21        \n"," stributed)                                                      \n","                                                                 \n","=================================================================\n","Total params: 51465 (201.04 KB)\n","Trainable params: 51465 (201.04 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["def rmse(y_true, y_pred):\n","    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n","\n","keras.backend.clear_session()\n","def build_generic_feature_model(input_shape):\n","    # Input for generic features\n","    input_shape = (24, X_batch.shape[2])\n","\n","    generic_input = Input(shape=input_shape)\n","\n","    # CNN-LSTM architecture\n","    conv_layer = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(generic_input)\n","    lstm_layer = LSTM(units=70, return_sequences=True, activation='relu')(conv_layer)\n","    lstm_layer_3 = Dropout(rate=0.4)(lstm_layer)\n","    # Dense layers for prediction\n","    time_distributed= TimeDistributed(Dense(20))(lstm_layer_3)\n","    time_distributed_output = TimeDistributed(Dense(1))(time_distributed)\n","\n","    model = Model(inputs=generic_input, outputs=time_distributed_output)\n","    model.compile(optimizer=RMSprop(learning_rate=0.001), loss='mse', metrics=['mae', rmse])\n","    model.summary()\n","    return model\n","\n","generic_feature_model = build_generic_feature_model(input_shape=(24, X_batch.shape[2]))"]},{"cell_type":"markdown","source":["# Running generic model"],"metadata":{"id":"b-aoY7KGyZDW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"H4visOGUwSSe"},"outputs":[],"source":["# Callback for early stopping\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dPVrrNVDwSSe"},"outputs":[],"source":["import os\n","from tensorflow.keras.models import load_model\n","\n","\n","checkpoint_dir = '/content/drive/MyDrive/Colab Notebooks/checkpoint_directory'\n","list_of_files = os.listdir(checkpoint_dir)\n","# Filter out files that match your checkpoint pattern and sort them\n","checkpoint_files = sorted([f for f in list_of_files if 'model_cnn-lstm-generic' in f])\n","\n","if checkpoint_files:\n","    last_checkpoint = checkpoint_files[-1]\n","    last_model_path = os.path.join(checkpoint_dir, last_checkpoint)\n","    # Load the last model with the custom rmse function\n","    generic_feature_model = load_model(last_model_path, custom_objects={'rmse': rmse})\n","else:\n","    print(\"No checkpoint found. Please train the model from scratch.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jmF32BocwSSf"},"outputs":[],"source":["# Configure ModelCheckpoint\n","\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","# Define ModelCheckpoint callback\n","checkpoint = ModelCheckpoint(\n","    '/content/drive/MyDrive/Colab Notebooks/checkpoint_directory/model_cnn-lstm-generic{epoch:02d}-{loss:.2f}.h5',\n","    monitor='loss',\n","    verbose=1,\n","    save_best_only=True,\n","    mode='min'\n",")\n","\n","steps_per_epoch = len(X_train) // batch_size\n","\n","validation_steps = len(X_val) // batch_size\n","\n","# Train the model using the generator\n","history = generic_feature_model.fit(\n","    x=train_gen_without_station_ids,\n","    steps_per_epoch=steps_per_epoch,\n","    epochs=50,\n","    validation_data=val_gen_without_station_ids,\n","    validation_steps=validation_steps,\n","    callbacks=[early_stopping, checkpoint]\n",")"]},{"cell_type":"markdown","source":["# Generating predictions"],"metadata":{"id":"aO2TxHRYyhrE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GyxUs7cAUC6D"},"outputs":[],"source":["from tensorflow.keras.models import load_model\n","\n","# Replace this path with the path to your model file\n","model_path =\"/content/drive/MyDrive/Colab Notebooks/checkpoint_directory/model_cnn-lstm-generic09-0.61.h5\"\n","\n","# Load the model\n","generic_feature_model_1 = load_model(model_path, custom_objects={'rmse': rmse})\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZB-TKyJ5UEmd"},"outputs":[],"source":["import numpy as np\n","\n","def reshape_and_pad_data(X, y, time_steps=24):\n","    # Calculate the padding size\n","    pad_size = time_steps - (X.shape[0] % time_steps)\n","    pad_size = pad_size if pad_size != time_steps else 0\n","\n","    # Pad the data\n","    X_padded = np.vstack([X, X.iloc[:pad_size]])\n","    y_padded = np.vstack([y, y.iloc[:pad_size]])\n","\n","    # Reshape the data\n","    X_reshaped = X_padded.reshape((-1, time_steps, X.shape[1]))\n","    y_reshaped = y_padded.reshape((-1, time_steps, 1))  # Add an extra dimension for y\n","\n","    return X_reshaped, y_reshaped\n","\n","# Reshape and pad test data\n","X_reshaped, y_reshaped = reshape_and_pad_data(X_test, y_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6980,"status":"ok","timestamp":1701360349981,"user":{"displayName":"Rob van der Wielen","userId":"16334711937746181770"},"user_tz":-60},"id":"-GJqVF0NUHA9","outputId":"6508c590-5a2f-4a75-aa53-dfcc6538604c"},"outputs":[{"output_type":"stream","name":"stdout","text":["1060/1060 [==============================] - 7s 6ms/step\n"]}],"source":["# Now station_id_batch_reshaped should be ready for use in prediction\n","test_predictions_batch = generic_feature_model_1.predict(X_reshaped)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3DGUrUybUN83"},"outputs":[],"source":["# Flatten the last two dimensions for both predictions and actuals\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from math import sqrt\n","y_test_flat = y_reshaped.reshape(-1)  # Reshape to (51240 * 24,)\n","test_predictions_flat = test_predictions_batch.reshape(-1)  # Reshape to (51240 * 24,)\n","\n","# Calculate MAE and RMSE\n","test_mae = mean_absolute_error(y_test_flat, test_predictions_flat)\n","test_rmse = sqrt(mean_squared_error(y_test_flat, test_predictions_flat))\n","\n","print('Test MAE:', test_mae)\n","print('Test RMSE:', test_rmse)\n"]},{"cell_type":"markdown","source":["# Exporting predictions"],"metadata":{"id":"-TDxBpgxyt2n"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qh1qzS5swrs1"},"outputs":[],"source":["def unpad_predictions(predictions, original_data, time_steps=24):\n","    # Calculate the pad size used during padding\n","    pad_size = time_steps - (original_data.shape[0] % time_steps)\n","    pad_size = pad_size if pad_size != time_steps else 0\n","\n","    # Remove the padding\n","    predictions_unpadded = predictions[:-pad_size] if pad_size != 0 else predictions\n","\n","    return predictions_unpadded\n","\n","# Unpad the predictions\n","unpadded_predictions = unpad_predictions(test_predictions_flat, y_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LkqnGEsCcrxb"},"outputs":[],"source":["# Convert the numpy array to a pandas DataFrame\n","unpadded_predictions_df = pd.DataFrame(unpadded_predictions, columns=['prediction'])\n","\n","# Export to a CSV file\n","unpadded_predictions_df.to_csv('/content/drive/MyDrive/Colab Notebooks/general_standardized_predictions_df_normal9.csv', index=False)\n"]},{"cell_type":"markdown","source":["# Exporting unstandardized predictions"],"metadata":{"id":"fyrPNAfeyzUD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gzsx3LCfw1sQ"},"outputs":[],"source":["# Retrieve the mean and variance from the target_standardizer\n","mean, variance = target_standardizer.mean.numpy(), target_standardizer.variance.numpy()\n","\n","# Compute the standard deviation from the variance\n","std_dev = np.sqrt(variance)\n","\n","# Extract single value for mean and standard deviation\n","mean_value = mean.item()  # Assuming mean is a numpy array with a single element\n","std_dev_value = np.sqrt(variance).item()  # Assuming variance is a numpy array with a single element\n","\n","# Apply the inverse transformation formula to unstandardize adjusted_test_predictions_flat\n","unstandardized_test_predictions_flat = unpadded_predictions * std_dev_value + mean_value\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uiF-Gme0x4_M"},"outputs":[],"source":["import pandas as pd\n","\n","# Convert the numpy array to a pandas DataFrame\n","predictions_df = pd.DataFrame(unstandardized_test_predictions_flat, columns=['prediction'])\n","\n","# Export to a CSV file\n","predictions_df.to_csv('/content/drive/MyDrive/Colab Notebooks/general_unstandardized_predictions.csv', index=False)\n"]},{"cell_type":"markdown","metadata":{"id":"OnrcS5otrj2f"},"source":["# Getting predictions for full dataset to substract from specific model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nllOXBbf95ie"},"outputs":[],"source":["import pandas as pd\n","merged_data_short = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/merged_data_short.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p2STerA97FCZ"},"outputs":[],"source":["# Define aggregation dictionary for variables other than 'start_count'\n","agg_funcs = {col: 'first' for col in merged_data_short.columns if col not in ['station_id_encoded', 'hour', 'date', 'start_count']}\n","agg_funcs['start_count'] = 'sum'\n","# Grouping the data\n","merged_data_short = merged_data_short.groupby(['station_id_encoded', 'hour', 'date'], as_index=False).agg(agg_funcs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vbubo5Z2Ujc6"},"outputs":[],"source":["aggregated_df = merged_data_short[[\"start_count\",\"date\", \"station_id_encoded\", \"hour\",\n","    \"temperature_2m (°C)\",\n","    \"relativehumidity_2m (%)\",\n","    \"precipitation (mm)\",\n","    \"snowfall (cm)\",\n","    \"cloudcover (%)\",\n","    \"IsHoliday\",\n","    \"direct_radiation (W/m²)\",\n","    \"windspeed_0m (km/h)\",\n","    \"year_2021\",\n","    \"year_2022\",\n","    \"year_2023\",\n","    \"month_name_April\",\n","    \"month_name_August\",\n","    \"month_name_December\",\n","    \"month_name_February\",\n","    \"month_name_January\",\n","    \"month_name_July\",\n","    \"month_name_June\",\n","    \"month_name_March\",\n","    \"month_name_May\",\n","    \"month_name_November\",\n","    \"month_name_October\",\n","    \"month_name_September\",\n","    \"day_of_week_Friday\",\n","    \"day_of_week_Monday\",\n","    \"day_of_week_Saturday\",\n","    \"day_of_week_Sunday\",\n","    \"day_of_week_Thursday\",\n","    \"day_of_week_Tuesday\",\n","    \"day_of_week_Wednesday\"]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"240QHzs4lPxL"},"outputs":[],"source":["# Sampling 50 unique station IDs at random\n","random_stations = aggregated_df['station_id_encoded'].drop_duplicates().sample(n=50, random_state=42)\n","\n","# Exclude these stations from the dataset\n","aggregated_df_sample = aggregated_df[~aggregated_df['station_id_encoded'].isin(random_stations)]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9SjQ5feYUxFr"},"outputs":[],"source":["# Sort by 'date' (and other relevant columns if necessary)\n","merged_data_short_sorted = aggregated_df_sample.sort_values(by=['date', 'station_id_encoded'])\n","# Reset index after sorting\n","merged_data_short_sorted.reset_index(drop=True, inplace=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tCpj5qyFpD4q"},"outputs":[],"source":["# Split sorted data into features (X) and target (y)\n","X_sorted = merged_data_short_sorted.drop('start_count', axis=1)\n","y_sorted = merged_data_short_sorted['start_count']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zHRtGY6oUeqM"},"outputs":[],"source":["import tensorflow as tf\n","# Feature columns to be standardized\n","feature_columns = [\"hour\", \"temperature_2m (°C)\",\n","                   \"relativehumidity_2m (%)\", \"precipitation (mm)\", \"snowfall (cm)\",\n","                   \"cloudcover (%)\", \"direct_radiation (W/m²)\", \"windspeed_10m (km/h)\"]\n","\n","# Create the Normalization layer and adapt it to the training data\n","standardizer = tf.keras.layers.Normalization(axis=-1)\n","standardizer.adapt(X_sorted[feature_columns])\n","\n","# Apply the standardizer to the training data\n","X_sorted[feature_columns] = standardizer(X_sorted[feature_columns].values)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qXDrX9JCU4DU"},"outputs":[],"source":["# Adapt the standardizer to the training data target variable\n","target_standardizer = tf.keras.layers.Normalization(axis=-1)\n","\n","target_standardizer.adapt(y_sorted.to_frame())\n","\n","# Apply the standardizer to the training data target variable\n","y_sorted = target_standardizer(y_sorted.to_frame()).numpy().flatten()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DdyD8TpFVBun"},"outputs":[],"source":["X_sorted.drop(columns=[\"station_id_encoded\"], inplace=True)\n","X_sorted.drop(columns=[\"date\"], inplace=True)\n","y_sorted = pd.DataFrame(y_sorted, columns=['start_count'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qaluasv_VEyS"},"outputs":[],"source":["from tensorflow.keras.models import load_model\n","\n","# Replace this path with the path to your model file\n","model_path = \"/content/drive/MyDrive/Colab Notebooks/checkpoint_directory/model_cnn-lstm-generic09-0.63.h5\"\n","\n","# Load the model\n","generic_feature_model_1 = load_model(model_path, custom_objects={'rmse': rmse})\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DOOwd5N_VCkv"},"outputs":[],"source":["import numpy as np\n","\n","def reshape_and_pad_data(X, y, time_steps=24):\n","    # Calculate the padding size\n","    pad_size = time_steps - (X.shape[0] % time_steps)\n","    pad_size = pad_size if pad_size != time_steps else 0\n","\n","    # Pad the data\n","    X_padded = np.vstack([X, X.iloc[:pad_size]])\n","    y_padded = np.vstack([y, y.iloc[:pad_size]])\n","\n","    # Reshape the data\n","    X_reshaped = X_padded.reshape((-1, time_steps, X.shape[1]))\n","    y_reshaped = y_padded.reshape((-1, time_steps, 1))  # Add an extra dimension for y\n","\n","    return X_reshaped, y_reshaped\n","\n","# Reshape and pad test data\n","X_reshaped, y_reshaped = reshape_and_pad_data(X_sorted, y_sorted)\n"]},{"cell_type":"markdown","source":["# Predict using full dataset"],"metadata":{"id":"d6mylhAB1_u2"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31577,"status":"ok","timestamp":1701196443147,"user":{"displayName":"Rob van der Wielen","userId":"16334711937746181770"},"user_tz":-60},"id":"wnlPgIq8WlAy","outputId":"ad8866a0-7ae4-428f-dd25-13ab58854a82"},"outputs":[{"name":"stdout","output_type":"stream","text":["4936/4936 [==============================] - 26s 5ms/step\n"]}],"source":["# Now station_id_batch_reshaped should be ready for use in prediction\n","test_predictions_batch = generic_feature_model_1.predict(X_reshaped)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MYcwMya2WmR0"},"outputs":[],"source":["# Flatten the last two dimensions for both predictions and actuals\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from math import sqrt\n","y_train_flat = y_reshaped.reshape(-1)  # Reshape to (51240 * 24,)\n","train_predictions_flat = test_predictions_batch.reshape(-1)  # Reshape to (51240 * 24,)\n","\n","# Calculate MAE and RMSE\n","test_mae = mean_absolute_error(y_train_flat, train_predictions_flat)\n","test_rmse = sqrt(mean_squared_error(y_train_flat, train_predictions_flat))\n","\n","print('Test MAE:', test_mae)\n","print('Test RMSE:', test_rmse)\n"]},{"cell_type":"markdown","source":["# Export predictions for later use"],"metadata":{"id":"2qrNgRAV2Dn1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hi7I9Lfze6z-"},"outputs":[],"source":["def unpad_predictions(predictions, original_data, time_steps=24):\n","    # Calculate the pad size used during padding\n","    pad_size = time_steps - (original_data.shape[0] % time_steps)\n","    pad_size = pad_size if pad_size != time_steps else 0\n","\n","    # Remove the padding\n","    predictions_unpadded = predictions[:-pad_size] if pad_size != 0 else predictions\n","\n","    return predictions_unpadded\n","\n","# Unpad the predictions\n","unpadded_predictions = unpad_predictions(train_predictions_flat, y_sorted)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e67Am33pLGPb"},"outputs":[],"source":["import pandas as pd\n","\n","# Convert the numpy array to a pandas DataFrame\n","unpadded_predictions_ = pd.DataFrame(unpadded_predictions, columns=['prediction'])\n","\n","# Export to a CSV file\n","unpadded_predictions_.to_csv('/content/drive/MyDrive/Colab Notebooks/predictions_full_generic_model_standardized_new.csv', index=False)\n"]},{"cell_type":"markdown","source":["## Export unstandardized predictions"],"metadata":{"id":"Y4-NoTDM2H36"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6-42RsGkmB4C"},"outputs":[],"source":["# Retrieve the mean and variance from the target_standardizer\n","mean, variance = target_standardizer.mean.numpy(), target_standardizer.variance.numpy()\n","\n","# Compute the standard deviation from the variance\n","std_dev = np.sqrt(variance)\n","\n","# Extract single value for mean and standard deviation\n","mean_value = mean.item()  # Assuming mean is a numpy array with a single element\n","std_dev_value = np.sqrt(variance).item()  # Assuming variance is a numpy array with a single element\n","\n","# Apply the inverse transformation formula to unstandardize adjusted_train_predictions_flat\n","unstandardized_train_predictions_flat = unpadded_predictions * std_dev_value + mean_value\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N7quJw-20oih"},"outputs":[],"source":["# Convert the numpy array to a pandas DataFrame\n","predictions_full_generic_model_train = pd.DataFrame(unstandardized_train_predictions_flat, columns=['prediction'])\n","\n","# Export to a CSV file\n","predictions_full_generic_model_train.to_csv('/content/drive/MyDrive/Colab Notebooks/predictions_full_generic_model_unstandardized_new.csv', index=False)\n"]},{"cell_type":"markdown","metadata":{"id":"eq5to9xiy70e"},"source":["# Specific model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fljK7sInT_B8"},"outputs":[],"source":["predictions_full_generic_model_train = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/predictions_full_generic_model_train.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZPueKOscpQuO"},"outputs":[],"source":["merged_data_short = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/merged_data_short.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4NNJCFiqOrpV"},"outputs":[],"source":["# Define aggregation dictionary for variables other than 'start_count'\n","agg_funcs = {col: 'first' for col in merged_data_short.columns if col not in ['station_id_encoded', 'hour', 'date', 'start_count']}\n","agg_funcs['start_count'] = 'sum'\n","\n","# Grouping the data\n","merged_data_short = merged_data_short.groupby(['station_id_encoded', 'hour', 'date'], as_index=False).agg(agg_funcs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g1sEMEGmy8x_"},"outputs":[],"source":["merged_data_short_specific = merged_data_short[[\"station_id_encoded\", \"start_count\", \"latitude\",\n","                                                \"longitude\", \"bike_lane_length_km\",\n","                                                \"restaurants_count\", \"rail_stations_count\",\n","                                                \"universities_count\", \"bus_stations_count\",\n","                                                \"parks_count\", \"date\", \"hour\"]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JtAO7OHipZnc"},"outputs":[],"source":["# Sampling 50 unique station IDs at random\n","random_stations = merged_data_short_specific['station_id_encoded'].drop_duplicates().sample(n=50, random_state=42)\n","\n","# Exclude these stations from the dataset\n","merged_data_short_specific = merged_data_short_specific[~merged_data_short_specific['station_id_encoded'].isin(random_stations)]\n"]},{"cell_type":"markdown","source":["## Splitting and standardizing data"],"metadata":{"id":"D0dpaa792i2X"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-8xohygXpfrZ"},"outputs":[],"source":["\n","merged_data_short_sorted = merged_data_short_specific.sort_values(by=['date', 'station_id_encoded'])\n","# Reset index after sorting\n","merged_data_short_sorted.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","source":["# Split sorted data into features (X) and target (y)\n","X_sorted = merged_data_short_sorted.drop('start_count', axis=1)\n","y_sorted = merged_data_short_sorted['start_count']\n","\n","# Split on date\n","train_end_index = X_sorted[X_sorted['date'] == '2022-11-30'].index[-1]\n","val_end_index = X_sorted[X_sorted['date'] == '2023-03-10'].index[-1]\n","\n","# Split the data manually\n","X_train = X_sorted.iloc[:train_end_index]\n","y_train = y_sorted.iloc[:train_end_index]\n","X_val = X_sorted.iloc[train_end_index:val_end_index]\n","y_val = y_sorted.iloc[train_end_index:val_end_index]\n","X_test = X_sorted.iloc[val_end_index:]\n","y_test = y_sorted.iloc[val_end_index:]"],"metadata":{"id":"r_olxm3v2YGr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dHOGSVyrPQHe"},"outputs":[],"source":["y_train = pd.DataFrame(y_train, columns=['start_count'])\n","y_test = pd.DataFrame(y_test, columns=['start_count'])\n","y_val = pd.DataFrame(y_val, columns=['start_count'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xratkv1yO76N"},"outputs":[],"source":["# Reference the predictions column\n","predictions_column = predictions_full_generic_model_train['prediction']\n","test = predictions_column.iloc[:train_end_index]"]},{"cell_type":"markdown","source":["### Substract predictions from train data for specific model"],"metadata":{"id":"MnfKV5Aq2aMe"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1uCLWedcOce9"},"outputs":[],"source":["# Ensure the lengths match\n","if len(y_train) == len(test):\n","    # Perform the subtraction\n","    y_train['start_count'] = y_train['start_count'] - test\n","else:\n","    print(\"Error: The lengths of the DataFrame and the predictions array do not match.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1mAEEo5ir-aO"},"outputs":[],"source":["X_train.drop(columns=[\"date\"], inplace=True)\n","X_val.drop(columns=[\"date\"], inplace=True)\n","X_test.drop(columns=[\"date\"], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MdyGJoIJr-aP"},"outputs":[],"source":["import tensorflow as tf\n","feature_columns = [\"latitude\", \"longitude\",\n","                   \"bike_lane_length_km\", \"restaurants_count\", \"rail_stations_count\",\n","                   \"universities_count\", \"bus_stations_count\", \"parks_count\"]\n","\n","\n","# Create the Normalization layer and adapt it to the training data\n","standardizer = tf.keras.layers.Normalization(axis=-1)\n","standardizer.adapt(X_train[feature_columns])\n","\n","# Apply the standardizer to the training data\n","X_train[feature_columns] = standardizer(X_train[feature_columns].values)\n","\n","# Apply the standardizer to the validation data\n","X_val[feature_columns] = standardizer(X_val[feature_columns].values)\n","\n","# Apply the standardizer to the test data\n","X_test[feature_columns] = standardizer(X_test[feature_columns].values)\n","\n","# Adapt the standardizer to the training data target variable\n","target_standardizer = tf.keras.layers.Normalization(axis=-1)\n","\n","target_standardizer.adapt(y_train)\n","\n","# Apply the standardizer to the training data target variable\n","y_train = target_standardizer(y_train).numpy().flatten()\n","\n","# Apply the standardizer to the validation data target variable\n","y_val = target_standardizer(y_val).numpy().flatten()\n","\n","# Apply the standardizer to the test data target variable\n","y_test = target_standardizer(y_test).numpy().flatten()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F6GBygB9r-aP"},"outputs":[],"source":["y_train = pd.DataFrame(y_train, columns=['start_count'])\n","y_test = pd.DataFrame(y_test, columns=['start_count'])\n","y_val = pd.DataFrame(y_val, columns=['start_count'])\n","\n","X_train.drop(columns=[\"hour\"], inplace=True)\n","X_val.drop(columns=[\"hour\"], inplace=True)\n","X_test.drop(columns=[\"hour\"], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eKgbtpJ3r-aP"},"outputs":[],"source":["# Number of unique stations in the dataset\n","num_stations = X_val['station_id_encoded'].nunique()\n","\n","# Maximum value of station ID (needed to define the input dimension of the embedding layer)\n","max_station_id = int(X_val['station_id_encoded'].max()) + 1\n","\n","# Define the embedding size\n","# Rule of thumb: min(50, number of categories/2)\n","embedding_size = min(50, num_stations // 2)\n","num_stations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aNiog0_-r-aP"},"outputs":[],"source":["station_ids_train = X_train['station_id_encoded']\n","station_ids_val = X_val['station_id_encoded']\n","station_ids_test = X_test['station_id_encoded']"]},{"cell_type":"markdown","source":["# Batch generator"],"metadata":{"id":"qSRHIZWE2yaw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"SLVsFlHSr-aQ"},"outputs":[],"source":["import numpy as np\n","\n","def modified_batch_generator(X, y, station_ids, time_steps=24, batch_size=256, infinite_loop=True):\n","    total_size = len(X)\n","    start_idx = 0\n","\n","    while True:\n","        X_batch = np.zeros((batch_size, time_steps, X.shape[1]))\n","        station_id_batch = np.zeros((batch_size, 1))\n","        y_batch = np.zeros((batch_size, time_steps, 1))\n","\n","        for i in range(batch_size):\n","            if start_idx + time_steps <= total_size:\n","                X_seq = X.iloc[start_idx:start_idx + time_steps]\n","                y_seq = y.iloc[start_idx:start_idx + time_steps]\n","                station_id = station_ids.iloc[start_idx + time_steps - 1]\n","\n","                # Padding in case of insufficient data\n","                X_pad = np.zeros((time_steps, X.shape[1]))\n","                y_pad = np.zeros((time_steps, 1))\n","                X_pad[:len(X_seq)] = X_seq.values\n","                y_pad[:len(y_seq)] = y_seq.values.reshape(-1, 1)\n","\n","                X_batch[i] = X_pad\n","                station_id_batch[i] = station_id\n","                y_batch[i] = y_pad\n","                start_idx += 1\n","            else:\n","                if infinite_loop:\n","                    start_idx = 0\n","                else:\n","                    break\n","\n","        yield ([X_batch, station_id_batch], y_batch)\n","\n","        if not infinite_loop and start_idx + time_steps > total_size:\n","            break\n","\n","# Recreate generators with the modified batch generator\n","train_gen = modified_batch_generator(X_train, y_train, station_ids_train, time_steps=24, batch_size=256, infinite_loop=True)\n","val_gen = modified_batch_generator(X_val, y_val, station_ids_val, time_steps=24, batch_size=256, infinite_loop=True)\n","test_gen = modified_batch_generator(X_test, y_test, station_ids_test, time_steps=24, batch_size=256, infinite_loop=False)\n","\n","batch_size = 256\n","# Get the next batch from the generator\n","batch_output = next(train_gen)\n","\n","# Extract station_id_batch, X_batch, and y_batch\n","X_batch, station_id_batch = batch_output[0]\n","y_batch = batch_output[1]\n","\n","# Print shapes\n","print(\"Station ID Batch Shape:\", station_id_batch.shape)\n","print(\"X Batch Shape:\", X_batch.shape)\n","print(\"y Batch Shape:\", y_batch.shape)"]},{"cell_type":"markdown","source":["## Specific model"],"metadata":{"id":"ObpvEnBJ21L5"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":556,"status":"ok","timestamp":1701422542744,"user":{"displayName":"Rob van der Wielen","userId":"16334711937746181770"},"user_tz":-60},"id":"Iu8gtaadr-aQ","outputId":"e79c4aec-9852-4aeb-8f33-132eb4e4c2b1"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_1 (InputLayer)        [(None, 24, 10)]             0         []                            \n","                                                                                                  \n"," conv1d (Conv1D)             (None, 24, 64)               3264      ['input_1[0][0]']             \n","                                                                                                  \n"," input_2 (InputLayer)        [(None, 1)]                  0         []                            \n","                                                                                                  \n"," dropout (Dropout)           (None, 24, 64)               0         ['conv1d[0][0]']              \n","                                                                                                  \n"," embedding (Embedding)       (None, 1, 50)                13250     ['input_2[0][0]']             \n","                                                                                                  \n"," lstm (LSTM)                 (None, 24, 70)               37800     ['dropout[0][0]']             \n","                                                                                                  \n"," flatten (Flatten)           (None, 50)                   0         ['embedding[0][0]']           \n","                                                                                                  \n"," dropout_1 (Dropout)         (None, 24, 70)               0         ['lstm[0][0]']                \n","                                                                                                  \n"," repeat_vector (RepeatVecto  (None, 24, 50)               0         ['flatten[0][0]']             \n"," r)                                                                                               \n","                                                                                                  \n"," concatenate (Concatenate)   (None, 24, 120)              0         ['dropout_1[0][0]',           \n","                                                                     'repeat_vector[0][0]']       \n","                                                                                                  \n"," time_distributed (TimeDist  (None, 24, 64)               7744      ['concatenate[0][0]']         \n"," ributed)                                                                                         \n","                                                                                                  \n"," time_distributed_1 (TimeDi  (None, 24, 1)                65        ['time_distributed[0][0]']    \n"," stributed)                                                                                       \n","                                                                                                  \n","==================================================================================================\n","Total params: 62123 (242.67 KB)\n","Trainable params: 62123 (242.67 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}],"source":["from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, concatenate, Conv1D, Flatten, Embedding, TimeDistributed, RepeatVector\n","from tensorflow.keras import regularizers\n","\n","def rmse(y_true, y_pred):\n","    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n","\n","cnn_lstm_input_shape = (24, X_batch.shape[2])\n","station_id_input_shape = (1,)\n","\n","def build_cnn_lstm_model_specific(cnn_lstm_input_shape, station_id_input_shape):\n","    cnn_lstm_input = Input(shape=cnn_lstm_input_shape)\n","    conv1 = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(cnn_lstm_input)\n","    lstm = Dropout(rate=0.2)(conv1)\n","    lstm1 = LSTM(units=70, activation='relu', return_sequences=True)(lstm)\n","    lstm2 = Dropout(rate=0.2)(lstm1)\n","\n","\n","    station_id_input = Input(shape=station_id_input_shape)\n","    station_id_embedding = Embedding(input_dim=max_station_id + 1, output_dim=embedding_size)(station_id_input)\n","    station_id_embedding = Flatten()(station_id_embedding)\n","    station_id_embedding = RepeatVector(24)(station_id_embedding)  # Repeat to match time steps\n","\n","    combined = concatenate([lstm2, station_id_embedding], axis=-1)\n","    time_distributed_output = TimeDistributed(Dense(64))(combined)\n","    final_output = TimeDistributed(Dense(1))(time_distributed_output)  # Predicting for each time step\n","\n","    optimizer = RMSprop(learning_rate=0.0001)\n","\n","    model = Model(inputs=[cnn_lstm_input, station_id_input], outputs=final_output)\n","    model.compile(optimizer=optimizer, loss='mse', metrics=['mae', rmse])\n","\n","    return model\n","\n","cnn_lstm_model_specific_no_cap = build_cnn_lstm_model_specific(cnn_lstm_input_shape, station_id_input_shape)\n","cnn_lstm_model_specific_no_cap.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mOVRTISsr-aQ"},"outputs":[],"source":["# Callback for early stopping\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TZUDGdbEr-aQ"},"outputs":[],"source":["checkpoint_dir = '/content/drive/MyDrive/Colab Notebooks/checkpoint_directory'\n","list_of_files = os.listdir(checkpoint_dir)\n","# Filter out files that match your checkpoint pattern and sort them\n","checkpoint_files = sorted([f for f in list_of_files if 'cnn_lstm_model_specific_no_cap' in f])\n","\n","if checkpoint_files:\n","    last_checkpoint = checkpoint_files[-1]\n","    last_model_path = os.path.join(checkpoint_dir, last_checkpoint)\n","    # Load the last model with the custom rmse function\n","    cnn_lstm_model_specific_no_cap = load_model(last_model_path, custom_objects={'rmse': rmse})\n","else:\n","    print(\"No checkpoint found. Please train the model from scratch.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BmSjSJ39r-aQ"},"outputs":[],"source":["\n","checkpoint = ModelCheckpoint(\n","    '/content/drive/MyDrive/Colab Notebooks/checkpoint_directory/cnn_lstm_model_specific_no_cap{epoch:02d}-{val_loss:.2f}.h5',\n","    monitor='val_loss',\n","    verbose=1,\n","    save_best_only=True,\n","    mode='min'\n",")\n","\n","steps_per_epoch = len(X_train) // batch_size\n","# Calculate the number of steps in the validation generator\n","validation_steps = len(X_val) // batch_size\n","\n","history = cnn_lstm_model_specific_no_cap.fit(\n","    x=train_gen,\n","    steps_per_epoch=steps_per_epoch,\n","    epochs=50,\n","    validation_data=val_gen,\n","    validation_steps=validation_steps,\n","    callbacks=[early_stopping, checkpoint]\n",")\n","\n"]},{"cell_type":"markdown","source":["## Generating predictions for specific model"],"metadata":{"id":"k5ICGxjJ3SbK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KxE7wD_HbfhP"},"outputs":[],"source":["from tensorflow.keras.models import load_model\n","\n","# Replace this path with the path to your model file\n","model_path = \"/content/drive/MyDrive/Colab Notebooks/checkpoint_directory/cnn_lstm_model_no_cap-2.23.h5\"\n","\n","# Load the model\n","specific_feature_model = load_model(model_path, custom_objects={'rmse': rmse})\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TMZ6EPc0XV9Q"},"outputs":[],"source":["import numpy as np\n","\n","def reshape_and_pad_data(X, y, time_steps=24):\n","    # Calculate the padding size\n","    pad_size = time_steps - (X.shape[0] % time_steps)\n","    pad_size = pad_size if pad_size != time_steps else 0\n","\n","    # Pad the data\n","    X_padded = np.vstack([X, X.iloc[:pad_size]])\n","    y_padded = np.vstack([y, y.iloc[:pad_size]])\n","\n","    # Reshape the data\n","    X_reshaped = X_padded.reshape((-1, time_steps, X.shape[1]))\n","    y_reshaped = y_padded.reshape((-1, time_steps, 1))  # Add an extra dimension for y\n","\n","    return X_reshaped, y_reshaped\n","\n","# Reshape and pad test data\n","X_test_reshaped, y_test_reshaped = reshape_and_pad_data(X_test, y_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w8x_t54Ybkg5"},"outputs":[],"source":["def replicate_and_reshape_station_ids(station_ids, num_sequences):\n","    # Calculate the exact number of replications needed\n","    num_replications = int(np.ceil(num_sequences / len(station_ids)))\n","\n","    # Replicate the station IDs\n","    station_ids_replicated = np.repeat(station_ids, num_replications)\n","\n","    # Trim the replicated array to match the exact number of sequences\n","    station_ids_trimmed = station_ids_replicated[:num_sequences]\n","\n","    # Reshape the station IDs to match the required format\n","    station_ids_reshaped = station_ids_trimmed.reshape((num_sequences, 1))\n","\n","    return station_ids_reshaped\n","\n","# Use the function to reshape station_id_batch\n","num_sequences = X_test_reshaped.shape[0]\n","station_id_batch_reshaped = replicate_and_reshape_station_ids(station_id_batch, num_sequences)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5632,"status":"ok","timestamp":1701423932381,"user":{"displayName":"Rob van der Wielen","userId":"16334711937746181770"},"user_tz":-60},"id":"YC6DIXghXV9R","outputId":"83845720-c4d3-41b3-a679-af1893e819cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["1060/1060 [==============================] - 5s 5ms/step\n"]}],"source":["# Now station_id_batch_reshaped should be ready for use in prediction\n","test_predictions_batch = specific_feature_model.predict([X_test_reshaped, station_id_batch_reshaped])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y09MnAcNXV9S"},"outputs":[],"source":["# Flatten the last two dimensions for both predictions and actuals\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from math import sqrt\n","y_test_flat = y_test_reshaped.reshape(-1)  # Reshape to (51240 * 24,)\n","test_predictions_flat = test_predictions_batch.reshape(-1)  # Reshape to (51240 * 24,)\n","\n","# Calculate MAE and RMSE\n","test_mae = mean_absolute_error(y_test_flat, test_predictions_flat)\n","test_rmse = sqrt(mean_squared_error(y_test_flat, test_predictions_flat))\n","\n","print('Test MAE:', test_mae)\n","print('Test RMSE:', test_rmse)\n"]},{"cell_type":"markdown","source":["# Export predictions"],"metadata":{"id":"7-oa2Y9_3jS2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"w1u08-wXyCLz"},"outputs":[],"source":["def unpad_predictions(predictions, original_data, time_steps=24):\n","    # Calculate the pad size used during padding\n","    pad_size = time_steps - (original_data.shape[0] % time_steps)\n","    pad_size = pad_size if pad_size != time_steps else 0\n","\n","    # Remove the padding\n","    predictions_unpadded = predictions[:-pad_size] if pad_size != 0 else predictions\n","\n","    return predictions_unpadded\n","\n","# Unpad the predictions\n","unpadded_predictions = unpad_predictions(test_predictions_flat, y_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gy7XUrWvyCL9"},"outputs":[],"source":["# Convert the numpy array to a pandas DataFrame\n","unpadded_predictions_ = pd.DataFrame(unpadded_predictions, columns=['prediction'])\n","\n","# Export to a CSV file\n","unpadded_predictions_.to_csv('/content/drive/MyDrive/Colab Notebooks/standardized_predictions_specific_model_cnn_lstm_model_no_cap.csv', index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xz6bkjCjzpeF"},"outputs":[],"source":["# Retrieve the mean and variance from the target_standardizer\n","mean, variance = target_standardizer.mean.numpy(), target_standardizer.variance.numpy()\n","\n","# Compute the standard deviation from the variance\n","std_dev = np.sqrt(variance)\n","\n","# Extract single value for mean and standard deviation\n","mean_value = mean.item()  # Assuming mean is a numpy array with a single element\n","std_dev_value = np.sqrt(variance).item()  # Assuming variance is a numpy array with a single element\n","\n","# Apply the inverse transformation formula to unstandardize adjusted_train_predictions_flat\n","unstandardized_test_predictions_flat = unpadded_predictions * std_dev_value + mean_value\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WroXpwfQi8r1"},"outputs":[],"source":["# Convert the numpy array to a pandas DataFrame\n","unstandardized_test_predictions_flat = pd.DataFrame(unstandardized_test_predictions_flat, columns=['prediction'])\n","unstandardized_test_predictions_flat.to_csv('/content/drive/MyDrive/Colab Notebooks/standardized_predictions_specific_model_cnn_lstm_model_no_cap.csv', index=False)\n"]},{"cell_type":"markdown","metadata":{"id":"Ejlp4Lg2Pl77"},"source":["# Result for splitted CNN-LSTM model"]},{"cell_type":"markdown","source":["## Import both predictions"],"metadata":{"id":"Bj_oW84e3qMd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"kF2GfCv50pN9"},"outputs":[],"source":["import pandas as pd\n","standardized_predictions_specific_model_test = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/unstandardized_predictions_specific_model_cnn_lstm_model_no_cap.csv\")\n","\n","import pandas as pd\n","standardized_predictions_general = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/general_standardized_predictions_df_normal8.csv\")\n","\n"]},{"cell_type":"markdown","source":["## Sum up the predictions"],"metadata":{"id":"ldl3Sd8-3uyG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-xerxwJd1gM7"},"outputs":[],"source":["# Convert DataFrame to a NumPy array and then perform matrix addition\n","if standardized_predictions_general.shape == standardized_predictions_specific_model_test.shape:\n","    result = standardized_predictions_general + standardized_predictions_specific_model_test\n","else:\n","    print(\"DataFrame and array do not have the same shape.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dr-nBh7m2UCt"},"outputs":[],"source":["result2 = result.to_numpy()"]},{"cell_type":"markdown","source":["## Generate performance"],"metadata":{"id":"HrIMN2tH30AH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BCs85pBs3b9E"},"outputs":[],"source":["# Flatten the last two dimensions for both predictions and actuals\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from math import sqrt\n","\n","# Calculate MAE and RMSE\n","test_mae = mean_absolute_error(y_test, result2)\n","test_rmse = sqrt(mean_squared_error(y_test, result2))\n","\n","print('Test MAE:', test_mae)\n","print('Test RMSE:', test_rmse)\n"]}]}
