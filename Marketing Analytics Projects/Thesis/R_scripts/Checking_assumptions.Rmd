---
title: "Checking_assumptions"
author: "Rob van der Wielen"
date: "2023-03-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#--- Install packages ---#

```{r}
install.packages("olsrr")
```

#--- Load libraries ---#
```{r}
library(readxl)
library(tidyverse)
library(olsrr)
library(moments)
```

#--- Loading in Data ---#

```{r}
netflix_data_complete <- read.csv("C:/Users/Systeembeheer/OneDrive - Tilburg University/Documenten/Master/Marketing Analytics/Thesis/ThesisRob/ThesisRob/Datasets/Cleaned_data/netflix_data_complete.csv", na.strings=c("", "NA"))

#remove column "X"
netflix_data_complete <- netflix_data_complete %>%
  select(!X)

summary(netflix_data_complete)
```


#--- Loading models ---#

*Loading in the models to check the assumptions*

# Full model (Model 2) without log transformation
```{r}
lmNetflix_2 = lm(cumulative_rank ~ 
               AVG_Nmpaa_comp + 
               AVG_Ngenre_comp +
               rating + 
               exclusive + 
               Type + 
               sequel+
               Drama+
               Comedy+
               Suspense+
               Family+
               Action+
               Sci.Fi+
               Romance+
               Other+
               mpaa_R_Restricted +
               mpaa_G_General_audiences +
               mpaa_PG_Parental_guidance_suggested +
               mpaa_NC17_Adults_only+
               mpaa_Not_Rated, data = netflix_data_complete) #Create the linear regression
summary(lmNetflix_2) #Review the results
```

# Full model (model 2) with log transformation
```{r}

# Specify the weight vector
lmNetflix_log_2 = lm(log(cumulative_rank) ~       
              AVG_Nmpaa_comp*Holiday_distance+ 
               AVG_Ngenre_comp*Holiday_distance + 
               rating + 
               exclusive + 
               Type + 
               sequel+
               Drama+
               Comedy+
               Suspense+
               Family+
               Action+
               Sci.Fi+
               Romance+
               Other+
               mpaa_R_Restricted +
               mpaa_G_General_audiences +
               mpaa_PG_Parental_guidance_suggested +
               mpaa_PG13_Parents_strongly_cautioned +
               mpaa_NC17_Adults_only+
               mpaa_Not_Rated, data = netflix_data_complete) #Create the linear regression
summary(lmNetflix_log_2) #Review the results
```


#--- Checking assumptions ---#

#--- linearity assumption ---#

# Linearity test on model 2 without log transformation
```{r}
pdf("plots/linearityModel_2.pdf") 
plot(lmNetflix_2)
# Close the pdf file
dev.off() 

```
*Does not look completely linear*

# Linearity test on model 2 with log transformation
```{r}
pdf("plots/linearityModel_log_2.pdf") 
plot(lmNetflix_log_2)
# Close the pdf file
dev.off() 

```


#---Normality assumption ---#

# Normality assumption, conducting a skewness test
```{r}
res2  <- resid(lmNetflix_log_2)
skewness(res2)
```
*The data a moderate negatively skewed distribution, which means that in the normal distribution, it has a longer left tail. But not higher than cutoff of 1 or -1*

#--- Multicollinearity ---#

# Checking for multicollinearity using VIF
```{r}
library(car)
VIF_scores <- as.data.frame(vif(lmNetflix_log_2))
VIF_scores
write.csv(VIF_scores, "VIF_scores.csv")

```
*Some values above VIF cut-off of 10. create a correlation matrix*

# correlation matrix
```{r}
res <- cor(netflix_data_complete[c(3:24)])

correlation_table <- as.data.frame(round(res, 2))

write.csv(correlation_table, "correlation_table.csv")


```
*seems like one of levels of the mpaa rating is causing the trouble. Because this is just a level of a control variable this level will be omitted from the analysis*


# Full model (model 2) with log transformation
```{r}

# Specify the weight vector
lmNetflix_log_2 = lm(log(cumulative_rank) ~       
              AVG_Nmpaa_comp*Holiday_distance+ 
               AVG_Ngenre_comp*Holiday_distance + 
               rating + 
               exclusive + 
               Type + 
               sequel+
               Drama+
               Comedy+
               Suspense+
               Family+
               Action+
               Sci.Fi+
               Romance+
               Other+
               mpaa_R_Restricted +
               mpaa_G_General_audiences +
               mpaa_PG_Parental_guidance_suggested +
               mpaa_NC17_Adults_only+
               mpaa_Not_Rated, data = netflix_data_complete) #Create the linear regression
summary(lmNetflix_log_2) #Review the results
```

#--- Influential observations ---#


# Studentized residuals to find outliers
```{r}
pdf("plots/outliers_plot1.pdf") 
sr0 <- rstudent(lmNetflix_log_2)

plot(sr0)
dev.off()
findBiggest <- function(x, n)
    as.numeric(names(sort(abs(x), decreasing = TRUE)[1 : n]))

badSr <- findBiggest(sr0, 1)
badSr


```
*Looks like only one is above the threshold of 3 (> threshold of three). Observation 269*

*additional check*
```{r}
pdf("plots/outliers_plot2.pdf") 
ols_plot_resid_stud(lmNetflix_log_2)
```
*No value above threshold*


# Index plot to fiend high leverage points
```{r}
pdf("plots/high_leverag1.pdf") 
lv0 <- hatvalues(lmNetflix_log_2)

lv0 <- hatvalues(lmNetflix_log_2)
# Plot the high leverage points using cut-off (p+1)/n
sample_size <- nrow(netflix_data_complete)
plot(lv0, pch=".", cex=2, main="High leverage points")  
text(x=1:length(lv0)+1, y=lv0, labels=ifelse(lv0>(length(coef(lmNetflix_log_2))+1)/sample_size, names(lv0),""), col="red")
dev.off()

```
*These values could be potential influential when the cutoff (p+1)/n is used. These are a lot*

# Additional check 
```{r}
pdf("plots/high_leverag_outliers.pdf") 
ols_plot_resid_lev(lmNetflix_log_2)
dev.off()
```
*Looks like that about 2 values are both outliers and have high leverage (72, 285)* 


# Cook's distance for influencial observations
```{r}
pdf("plots/Cooks_D.pdf") 

cooksd <- cooks.distance(lmNetflix_log_2)

# Plot the Cook's Distance using the traditional 4/n criterion
sample_size <- nrow(netflix_data_complete)
plot(cooksd, pch=".", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
abline(h = 4/sample_size, col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4/sample_size, names(cooksd),""), col="red")
dev.off()

badCd <- findBiggest(cooksd, 25)
badCd

```
*25 observations as influential*


# DFFITS
```{r}
pdf("plots/DFFITS.pdf") 

ols_plot_dffits(lmNetflix_log_2)
dev.off()

```
*also 25 observations as influential*

#--- Analysis without outliers ---#

*remove three most influential*
```{r}
model_clean <- update(lmNetflix_log_2, data = netflix_data_complete[-c(72, 269, 285), ])
summary(model_clean)

```
*The R-squared from the cleaned model is larger as are all of the slopes. So these three observations will be removed from the data set*

```{r}
model_clean <- update(lmNetflix_log_2, data = netflix_data_complete[-c(badCd), ])
summary(model_clean)

```

#--- Remove influencial observations ---#

```{r}
netflix_data_complete <- netflix_data_complete[-c(72, 269, 285), ]
```


#--- Plot influential observations ---#

```{r}
pdf("plots/cooksd2.pdf") 
cooksd <- cooks.distance(lmNetflix_log_2)

# Plot the Cook's Distance using the traditional 4/n criterion
sample_size <- nrow(netflix_data_complete)
plot(cooksd, pch=".", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>0.04, names(cooksd),""), col="red")
dev.off() 
```
#--- Write final dataset to csv ---#

```{r}
write.csv(netflix_data_complete, "C:/Users/Systeembeheer/OneDrive - Tilburg University/Documenten/Master/Marketing Analytics/Thesis/ThesisRob/ThesisRob/Datasets/Cleaned_data/RegressionData.csv")
```


